# -*- coding: utf-8 -*-
"""naive-bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/JeanMusenga/be81ed619cfc2a4e5c8c1627f9c1e931/naive-bayes.ipynb
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
#from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split

dataset=pd.read_excel('ARPs_and_ProgrammingPosts.xlsx')

x = dataset.Post
y = dataset.Class
X_train, X_test, y_train, y_test = train_test_split(x , y, test_size=0.33, random_state=125)


print('Total number of sample:', len(dataset))
# check the shape of X_train and X_test
print('X Training sample:', len(X_train))
print('X Test sample:', len(X_test))

# check the shape of y_train and y_test
print("-------------------------")
print('Y Training sample:', len(y_train))
print('Y Test sample:', len(y_test))

#Feature Engineering or feature generation/extraction is the process of transforming raw data into useful features that help us to understand
#our model better and increase its predictive power. I will carry out feature engineering on different types of variables.
from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer().fit(X_train) #CountVectorizer implements both tokenization and occurrence counting in a single class
X_train_vect = vect.transform(X_train).toarray()
X_test_vect = vect.transform(X_test).toarray()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

nb_model = GaussianNB()
nb_model = nb_model.fit(X_train_vect, y_train)

y_pred = nb_model.predict(X_test_vect)

print(classification_report(y_test, y_pred))